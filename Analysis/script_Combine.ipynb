{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Instructors:*\n",
    "*This Notebook uses the Combine.pl script of the Cosmic Ray e-Lab analyses to provide material for the following learning goals:*\n",
    "\n",
    "* *Combining blocks of data in different ways to investigate different questions*\n",
    "* *Understanding the problem of \"background\" data in scientific observation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation: Combining cosmic ray data to improve neutrino experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underneath the black tape covering each of the four detector panels of QuarkNet's cosmic ray muon detectors (CRMDs) is a clear plastic material called a [**scintillator**](https://en.wikipedia.org/wiki/Scintillator).  Scintillators are a broad class of materials defined by the property that they emit light as charged particles -- like cosmic ray muons -- pass through them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fig. 1 shows how many neutrino detection experiments work on a similar principle, except that these phantom particles interact so rarely that a much larger amount of scintillator is required to see any evidence of them.  Rather than a small plastic panel, these experiments use large tanks filled with thousands of tons of liquid scintillator.  And since neutrinos are uncharged, the scintillator material in these experiments does not respond to them directly, but rather to the charged particles (electrons and sometimes muons) that are \"kicked out\" on those rare occasions when a neutrino does interact with the nucleus of an atom of scintillator.\n",
    "\n",
    "These charged particle *do* leave glowing tracks in the scintillator, and photomultiplier tubes (PMTs) placed around the tank record that light.  Investigators can then use the properties of the light signal to make inferences about the neutrino that started the chain of events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![A neutrino detector](img/TankDetector-Beam_838x363.png)*Fig 1.  A beam of neutrinos passes through a tank of scintillator, occasionally kicking out a charged particle whose scintillation tracks can be seen by a PMT.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This detector design works well, but Fig. 2 shows one problem with it.  We know that cosmic ray muons strike the earth from the upper atmosphere constantly, so unless the detector is located deep underground (and some are!), then some cosmic rays muons will pass through the tank and leave scintillation tracks that register as data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![A muon striking a neutrino detector](img/TankDetector-Muon_838x418.png)*Fig 2.  A cosmic ray muon passes through the detector, leaving a track that might be mistaken for a particle produced by a neutrino event.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's bad.  Neutrino events are rare, and the presence of these extra muons makes the few true neutrino interactions that occur even harder to spot.  Almost all experiments have to deal with this sort of unwanted data that, at least on the surface, looks the same as the data that investigators really want to collect.  This \"false-positive\" data is called **background**, and understanding it is a major part of the design and interpretation of scientific experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, there are solutions to dealing with background.  Fig. 3 shows one example: we can place muon detectors on the top and bottom of the tank of scintillator.  When a cosmic ray muon passes through the tank, it will trigger CRMDs on both the top and bottom of the tank in rapid succession - almost instantaneously, in fact, since the muons are travelling close to the speed of light.  Particle tracks that originate within the tank, however, won't ever activate detectors on both sides of the tank at once.  This means that we can use software or electronic hardware to look for two closely-spaced CRMD events and prevent any signal in the detector tank from being recorded when that happens.  This rejection of bad data before it can be recorded is called a **veto**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![A neutrino detector with a veto system](img/TankDetector-Veto_838x432.png)*Fig 3.  A background muon will strike two CRMDs on opposite sides of the tank as it passes through, while a neutrino-produced particle within the tank will not.  This lets us \"filter out,\" or *veto*, cosmic ray muons from the data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for this to work, however, we need a way to combine the data from all of the CRMDs into a single block so that we can quickly look for nearly simultaneous events in two different detectors.  This is the role played by the `Combine` data transformation script.  We'll study two slightly different applications of this data tranformation: combining data sets from a single detector that were recorded at different times, and combining data from multiple detectors taken at the same time, as happens in the neutrino detector veto system we just explored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building blocks of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a scientific experiment, more signal data leads to more reliable results.  That doesn't mean that we always want all possible data in one big chunk, though.  Not only are large data sets hard to store and transfer, but patterns in relationships between two variables can be hard to see if they're mixed in with data on unrelated variables.  Storing and analyzing scientific data in smaller units gives us the flexibility to combine the data in different ways in order to investigate different questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same is true of the cosmic ray data generated by QuarkNet's CRMDs.  This data is stored in individual files, with each file representing data taken by a single detector over a time period of up to 24 hours.  For some studies -- say, investigating changes in cosmic ray flux as a function of fluctuations in atmospheric pressure -- we might want to look at data taken by a single detector over the course of several days.  In other studies -- say, searching for showers of cosmic rays caused by especially high-energy primary rays -- we might want to look at data taken by many detectors covering a wide geographic area, but only over the course of a few hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The e-Lab studies use the `Combine.pl` script for workflows that feature multiple input files.  As you might guess from its name, it combines data from two (or more) files into a single dataset that's contained in a single file.  This gives us the flexibility to examine the same set of raw data files in different ways depending on what variables are relevant to a given question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Combine.pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the data transformation script `Combine.pl`, we provide it with any number of input files followed by what we want it to name the output file it creates:\n",
    "\n",
    "`$ perl ./perl/Combine.pl <input file 1> <input file 2> ... <input file N> <output file>`\n",
    "\n",
    "where the items in angled brackets `<>` are parameters we have to specify.  These are:\n",
    "\n",
    "* `input file`:  The name of a file to be used as input; we can specify as many as we like\n",
    "* `output file`: What we want to name the output file that the script will write its results to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Combining data over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QuarkNet's cosmic ray muon detectors can take data continuously, and it's not unusual for users to run their detectors for days at a time.  For example, DAQ 6148 generated the three Threshold files\n",
    "\n",
    "`6148.2018.0602.0.thresh`\n",
    "\n",
    "`6148.2018.0603.0.thresh`\n",
    "\n",
    "`6148.2018.0604.0.thresh`\n",
    "\n",
    "over the 3-day period from June 2 to June 4, 2018.  The Cosmic Ray e-Lab breaks such data up into individual days for the sake of organization and to keep files from becoming unmanageably large.  When running studies on the data, though, we may want to consider the entire run all at once as a single dataset.  The `Combine` data transformation lets us do that.  We'll apply it to the above three files to see how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Investigating the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we'll start by examining the input data before we apply any transformation to it.  The following UNIX shell commands will use the `head` and `tail` programs to let us look at the three header lines plus the first 5 and last 5 data lines of each of these files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, for the file `6148.2018.0602.0.thresh`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#$md5\r\n",
      "#md5_hex(0)\r\n",
      "#ID.CHANNEL, Julian Day, RISING EDGE(sec), FALLING EDGE(sec), TIME OVER THRESHOLD (nanosec), RISING EDGE(INT), FALLING EDGE(INT)\r\n",
      "6148.2\t2458271\t0.5001851099136574\t0.5001851099139322\t23.74\t4321599349654000\t4321599349656374\r\n",
      "6148.2\t2458271\t0.5001851099139611\t0.5001851099141927\t20.01\t4321599349656624\t4321599349658625\r\n",
      "6148.3\t2458271\t0.5001851099140625\t0.5001851099144532\t33.76\t4321599349657500\t4321599349660876\r\n",
      "6148.2\t2458271\t0.5002396629087673\t0.5002396629089699\t17.51\t4322070687531750\t4322070687533500\r\n",
      "6148.3\t2458271\t0.5002396629091435\t0.5002396629093895\t21.26\t4322070687535000\t4322070687537125\r\n",
      "...\r\n",
      "6148.4\t2458272\t0.4996444257415654\t0.4996444257419126\t30.00\t4316927838407125\t4316927838410125\r\n",
      "6148.2\t2458272\t0.4997058953934896\t0.4997058953937355\t21.24\t4317458936199750\t4317458936201875\r\n",
      "6148.3\t2458272\t0.4997058953939526\t0.4997058953940539\t8.75\t4317458936203750\t4317458936204625\r\n",
      "6148.2\t2458272\t0.4998563698267506\t0.4998563698271123\t31.25\t4318759035303125\t4318759035306250\r\n",
      "6148.3\t2458272\t0.4998563698271846\t0.4998563698274595\t23.75\t4318759035306875\t4318759035309250\r\n"
     ]
    }
   ],
   "source": [
    "!head -8 files/6148.2018.0602.0.thresh ; echo \"...\" ; tail -5 files/6148.2018.0602.0.thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for the file `6148.2018.0603.0.thresh`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#$md5\r\n",
      "#md5_hex(0)\r\n",
      "#ID.CHANNEL, Julian Day, RISING EDGE(sec), FALLING EDGE(sec), TIME OVER THRESHOLD (nanosec), RISING EDGE(INT), FALLING EDGE(INT)\r\n",
      "6148.2\t2458272\t0.5002601237136719\t0.5002601237139034\t20.00\t4322247468886125\t4322247468888125\r\n",
      "6148.3\t2458272\t0.5002601237140480\t0.5002601237144098\t31.25\t4322247468889375\t4322247468892500\r\n",
      "6148.2\t2458272\t0.5002869503800058\t0.5002869503802229\t18.75\t4322479251283250\t4322479251285125\r\n",
      "6148.3\t2458272\t0.5002869503804398\t0.5002869503808015\t31.25\t4322479251287000\t4322479251290125\r\n",
      "6148.2\t2458272\t0.5003048243718605\t0.5003048243721354\t23.75\t4322633682572875\t4322633682575250\r\n",
      "...\r\n",
      "6148.3\t2458273\t0.4998297218178096\t0.4998297218180411\t20.00\t4318528796505875\t4318528796507875\r\n",
      "6148.2\t2458273\t0.4999118278728298\t0.4999118278731626\t28.75\t4319238192821250\t4319238192824125\r\n",
      "6148.2\t2458273\t0.4999118278731915\t0.4999118278733796\t16.25\t4319238192824374\t4319238192826000\r\n",
      "6148.4\t2458273\t0.4999118278733073\t0.4999118278734520\t12.50\t4319238192825375\t4319238192826625\r\n",
      "6148.3\t2458273\t0.4999118278733652\t0.4999118278736256\t22.50\t4319238192825875\t4319238192828126\r\n"
     ]
    }
   ],
   "source": [
    "!head -8 files/6148.2018.0603.0.thresh ; echo \"...\" ; tail -5 files/6148.2018.0603.0.thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for the file `6148.2018.0604.0.thresh`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#$md5\r\n",
      "#md5_hex(0)\r\n",
      "#ID.CHANNEL, Julian Day, RISING EDGE(sec), FALLING EDGE(sec), TIME OVER THRESHOLD (nanosec), RISING EDGE(INT), FALLING EDGE(INT)\r\n",
      "6148.2\t2458273\t0.5003691139060619\t0.5003691139063513\t25.01\t4323189144148375\t4323189144150875\r\n",
      "6148.3\t2458273\t0.5003691139064960\t0.5003691139068287\t28.75\t4323189144152125\t4323189144155000\r\n",
      "6148.2\t2458273\t0.5004505377767506\t0.5004505377769820\t19.99\t4323892646391125\t4323892646393124\r\n",
      "6148.3\t2458273\t0.5004505377772280\t0.5004505377775318\t26.24\t4323892646395250\t4323892646397875\r\n",
      "6148.2\t2458273\t0.5004672020113861\t0.5004672020117332\t30.00\t4324036625378375\t4324036625381375\r\n",
      "...\r\n",
      "6148.3\t2458274\t0.4997994782379630\t0.4997994782382378\t23.75\t4318267491976000\t4318267491978375\r\n",
      "6148.1\t2458274\t0.4998511005815972\t0.4998511005818142\t18.75\t4318713509025000\t4318713509026875\r\n",
      "6148.3\t2458274\t0.4998511005818432\t0.4998511005820891\t21.25\t4318713509027125\t4318713509029250\r\n",
      "6148.2\t2458274\t0.4999579281848959\t0.4999579281853009\t34.99\t4319636499517500\t4319636499521000\r\n",
      "6148.3\t2458274\t0.4999579281853588\t0.4999579281855758\t18.75\t4319636499521500\t4319636499523375\r\n"
     ]
    }
   ],
   "source": [
    "!head -8 files/6148.2018.0604.0.thresh ; echo \"...\" ; tail -5 files/6148.2018.0604.0.thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the clock system the DAQ uses to record `RISING EDGE` and `FALLING EDGE` times, the value `0.5` represents midnight in [Coordinated Universal Time](https://en.wikipedia.org/wiki/Coordinated_Universal_Time) (UTC).  We see that the first file, `6148.2018.0602.0.thresh`, begins just after UTC midnight on the [Julian day](https://en.wikipedia.org/wiki/Julian_day) 2458271, which is 12:00 AM UTC on June 2, 2018, and it ends just before midnight on the Julian day 2458272, which is 12:00 AM UTC on June 3, 2018. You can see that the next two files continue this pattern into the next two days.\n",
    "\n",
    "*You can find many Julian day converters online, such as [this one](http://www.onlineconversion.com/julian_date.htm), if you'd like to explore more*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the number of lines in each file using the UNIX utility `wc` (short for \"word count\") with the `-l` flag to count lines instead of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23055 files/6148.2018.0602.0.thresh\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l files/6148.2018.0602.0.thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23468 files/6148.2018.0603.0.thresh\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l files/6148.2018.0603.0.thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23449 files/6148.2018.0604.0.thresh\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l files/6148.2018.0604.0.thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Applying the data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `Combine` data transformation to join these three test files into a single dataset, calling it `combineOut-6148`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!perl ./perl/Combine.pl files/6148.2018.0602.0.thresh files/6148.2018.0603.0.thresh files/6148.2018.0604.0.thresh outputs/combineOut-6148"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Investigating the output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll examine the output file the same way we did for the input file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#16457cb3f02c44fe538b5c73ef984126\r\n",
      "#md5_hex(1528996872 1529340168 1529340168 1529340169  files/6148.2018.0602.0.thresh files/6148.2018.0603.0.thresh files/6148.2018.0604.0.thresh)\r\n",
      "#Combined data for files: files/6148.2018.0602.0.thresh files/6148.2018.0603.0.thresh files/6148.2018.0604.0.thresh \r\n",
      "6148.2\t2458271\t0.5001851099136574\t0.5001851099139322\t23.74\t4321599349654000\t4321599349656374\r\n",
      "6148.2\t2458271\t0.5001851099139611\t0.5001851099141927\t20.01\t4321599349656624\t4321599349658625\r\n",
      "6148.3\t2458271\t0.5001851099140625\t0.5001851099144532\t33.76\t4321599349657500\t4321599349660876\r\n",
      "6148.2\t2458271\t0.5002396629087673\t0.5002396629089699\t17.51\t4322070687531750\t4322070687533500\r\n",
      "6148.3\t2458271\t0.5002396629091435\t0.5002396629093895\t21.26\t4322070687535000\t4322070687537125\r\n",
      "...\r\n",
      "6148.3\t2458274\t0.4997994782379630\t0.4997994782382378\t23.75\t4318267491976000\t4318267491978375\r\n",
      "6148.1\t2458274\t0.4998511005815972\t0.4998511005818142\t18.75\t4318713509025000\t4318713509026875\r\n",
      "6148.3\t2458274\t0.4998511005818432\t0.4998511005820891\t21.25\t4318713509027125\t4318713509029250\r\n",
      "6148.2\t2458274\t0.4999579281848959\t0.4999579281853009\t34.99\t4319636499517500\t4319636499521000\r\n",
      "6148.3\t2458274\t0.4999579281853588\t0.4999579281855758\t18.75\t4319636499521500\t4319636499523375\r\n"
     ]
    }
   ],
   "source": [
    "!head -8 outputs/combineOut-6148; echo \"...\" ; tail -5 outputs/combineOut-6148"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see at a glance that the first five data lines of the output are the first five data lines of the first input file, `6148.2018.0602.0.thresh`, and the last five data lines of the output are the same as the last five data lines of the last input file, `6148.2018.0604.0.thresh`.  Using the [converter](http://www.onlineconversion.com/julian_date.htm) that we linked to earlier, we can see from the Julian day and first `RISING EDGE` times of the first and last data lines that the output file seems to cover a timespan of\n",
    "\n",
    "```\n",
    "2458271\t0.5001851099136574   ->   00:00:16 UTC, 2 June 2018\n",
    "2458274\t0.4999579281853588   ->   23:59:57 UTC, 4 June 2018\n",
    "```\n",
    "\n",
    "That's what we'd expect for a transformation that adds files from June 2, June 3, and June 4 together one after the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going further, we can check the line count of the output file just as we did for the input files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69966 outputs/combineOut-6148\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l outputs/combineOut-6148"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 69966 total lines in the output file.  Is this number what you expect it to be?  Note that it's almost, but not quite, equal to the sum of the line counts of each of the individual input files that we found earlier,\n",
    "\n",
    "```\n",
    "6148.2018.0602.0.thresh:        23055\n",
    "6148.2018.0603.0.thresh:        23468\n",
    "6148.2018.0604.0.thresh:      + 23449\n",
    "                                -----\n",
    "                                69972\n",
    "```\n",
    "\n",
    "Why are the two counts not exactly equal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Combining data over place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Combine` data transformation script also allows us to join data from two different sources into a single dataset.  For example, we may wish to look at data taken at the same time by two nearby detectors to see if there are any correlations in muon events between them.  This happens, for example, during a cosmic ray \"shower\" that results from a high-energy primary cosmic ray hitting the upper atmosphere and creating a large number of secondary cosmic rays in a cone that can cover up to a couple of square miles of the earth's surface.  When such a shower occurs, muon detectors within that cone will all register increases in muon counts at the same (or very close to the same) time.  By combining data from multiple detectors, we can reconstruct the direction that the shower-causing primary cosmic ray hit the earth from, and we can even get a rough idea of its total energy.\n",
    "\n",
    "QuarkNet's [Cosmic Ray e-Lab](https://www.i2u2.org/elab/cosmic/home/project.jsp) features an analysis tool for conducting studies on cosmic ray showers using uploaded CRMD data.  You can learn more about cosmic ray showers there, but for now we'll focus on how the `Combine` data transformation script works when combining data from multiple detectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try that with the following two files containing data taken by detectors 6148 and 6478 on May 7, 2017:\n",
    "\n",
    "`6148.2017.0507.0.thresh`\n",
    "\n",
    "`6478.2017.0507.0.thresh`\n",
    "\n",
    "Both of these detectors were located at [Fermilab](https://www.fnal.gov), so it's reasonable to suspect that if a cosmic ray shower occurred in the area, both detectors would have recorded it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Investigating the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do the usual trick with the UNIX `head` and `tail` commands to see the beginning and end of each file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the file `6148.2017.0507.0.thresh`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#$md5\r\n",
      "#md5_hex(0)\r\n",
      "#ID.CHANNEL, Julian Day, RISING EDGE(sec), FALLING EDGE(sec), TIME OVER THRESHOLD (nanosec), RISING EDGE(INT), FALLING EDGE(INT)\r\n",
      "6148.1\t2457880\t0.5017287926934751\t0.5017287926938802\t34.99\t4334936768871625\t4334936768875125\r\n",
      "6148.1\t2457880\t0.5017287926939091\t0.5017287926941262\t18.75\t4334936768875375\t4334936768877250\r\n",
      "6148.4\t2457880\t0.5017287926940828\t0.5017287926943577\t23.75\t4334936768876876\t4334936768879250\r\n",
      "6148.2\t2457880\t0.5030325167516059\t0.5030325167520978\t42.50\t4346200944733875\t4346200944738125\r\n",
      "6148.1\t2457880\t0.5030325167519096\t0.5030325167522569\t30.00\t4346200944736500\t4346200944739500\r\n",
      "...\r\n",
      "6148.2\t2457881\t0.4991468762158276\t0.4991468762161170\t25.00\t4312629010504750\t4312629010507250\r\n",
      "6148.1\t2457881\t0.4991468762158854\t0.4991468762162615\t32.49\t4312629010505250\t4312629010508500\r\n",
      "6148.2\t2457881\t0.4996565376225549\t0.4996565376225984\t3.76\t4317032485058874\t4317032485059250\r\n",
      "6148.3\t2457881\t0.4996565376228299\t0.4996565376230469\t18.75\t4317032485061250\t4317032485063125\r\n",
      "6148.3\t2457881\t0.4996565376231048\t0.4996565376232638\t13.75\t4317032485063625\t4317032485065000\r\n"
     ]
    }
   ],
   "source": [
    "!head -8 files/6148.2017.0507.0.thresh; echo \"...\" ; tail -5 files/6148.2017.0507.0.thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the file `6478.2017.0507.0.thresh`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#$md5\r\n",
      "#md5_hex(0)\r\n",
      "#ID.CHANNEL, Julian Day, RISING EDGE(sec), FALLING EDGE(sec), TIME OVER THRESHOLD (nanosec), RISING EDGE(INT), FALLING EDGE(INT)\r\n",
      "6478.2\t2457880\t0.5801056986752026\t0.5801056986755063\t26.24\t5012113236553750\t5012113236556375\r\n",
      "6478.1\t2457880\t0.5801056986751737\t0.5801056986756077\t37.50\t5012113236553501\t5012113236557250\r\n",
      "6478.3\t2457880\t0.5801059826235389\t0.5801059826239294\t33.75\t5012115689867375\t5012115689870750\r\n",
      "6478.4\t2457880\t0.5801059826234953\t0.5801059826238426\t30.00\t5012115689867000\t5012115689870000\r\n",
      "6478.4\t2457880\t0.5801063615076100\t0.5801063615078559\t21.25\t5012118963425750\t5012118963427875\r\n",
      "...\r\n",
      "6478.4\t2457881\t0.1316323887125434\t0.1316323887129051\t31.25\t1137303838476375\t1137303838479500\r\n",
      "6478.1\t2457881\t0.1316350724972946\t0.1316350724976128\t27.50\t1137327026376625\t1137327026379375\r\n",
      "6478.2\t2457881\t0.1316350724973235\t0.1316350724975984\t23.75\t1137327026376875\t1137327026379250\r\n",
      "6478.2\t2457881\t0.1316383624849537\t0.1316383624852720\t27.50\t1137355451870000\t1137355451872750\r\n",
      "6478.1\t2457881\t0.1316383624849248\t0.1316383624853154\t33.75\t1137355451869750\t1137355451873125\r\n"
     ]
    }
   ],
   "source": [
    "!head -8 files/6478.2017.0507.0.thresh; echo \"...\" ; tail -5 files/6478.2017.0507.0.thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first file, `6148.2017.0507.0.thresh`, covers the whole UTC day midnight-to-midnight:\n",
    "\n",
    "```\n",
    "(First data line)     2457880\t0.5017287926934751   ->   00:02:29 UTC, 7 May 2017\n",
    "(Last data line)      2457881\t0.4996565376231048   ->   23:59:31 UTC, 7 May 2017\n",
    "```\n",
    "\n",
    "The second file, `6478.2017.0507.0.thresh`, covers a time period fully within that of the first:\n",
    "\n",
    "```\n",
    "(First data line)     2457880\t0.5801056986752026   ->   01:55:22 UTC, 7 May 2017\n",
    "(Last data line)      2457881\t0.1316323887125434   ->   15:09:33 UTC, 7 May 2017\n",
    "```\n",
    "\n",
    "That is, a little before 2AM to a little after 3PM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll take a look at the line counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4424 files/6148.2017.0507.0.thresh\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l files/6148.2017.0507.0.thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7776 files/6478.2017.0507.0.thresh\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l files/6478.2017.0507.0.thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat interestingly, we note that even though detector 6478 took data for only about 13 hours, it recorded many more lines of data than the nearby detector 6148, which ran for the full 24-hour day on May 7, 2017.\n",
    "\n",
    "**Discussion:** What do you think might explain this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Applying the data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll feed the two input files we've selected into the data transformation script `Combine`, calling the output file `combineOut-05072017`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!perl ./perl/Combine.pl files/6148.2017.0507.0.thresh files/6478.2017.0507.0.thresh outputs/combineOut-05072017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Investigating the output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we examine the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#90995444efbf33e84090296aa80383c4\r\n",
      "#md5_hex(1528996872 1498068825 1494356148  files/6148.2017.0507.0.thresh files/6478.2017.0507.0.thresh)\r\n",
      "#Combined data for files: files/6148.2017.0507.0.thresh files/6478.2017.0507.0.thresh \r\n",
      "6148.1\t2457880\t0.5017287926934751\t0.5017287926938802\t34.99\t4334936768871625\t4334936768875125\r\n",
      "6148.1\t2457880\t0.5017287926939091\t0.5017287926941262\t18.75\t4334936768875375\t4334936768877250\r\n",
      "6148.4\t2457880\t0.5017287926940828\t0.5017287926943577\t23.75\t4334936768876876\t4334936768879250\r\n",
      "6148.2\t2457880\t0.5030325167516059\t0.5030325167520978\t42.50\t4346200944733875\t4346200944738125\r\n",
      "6148.1\t2457880\t0.5030325167519096\t0.5030325167522569\t30.00\t4346200944736500\t4346200944739500\r\n",
      "...\r\n",
      "6478.4\t2457881\t0.1316323887125434\t0.1316323887129051\t31.25\t1137303838476375\t1137303838479500\r\n",
      "6478.1\t2457881\t0.1316350724972946\t0.1316350724976128\t27.50\t1137327026376625\t1137327026379375\r\n",
      "6478.2\t2457881\t0.1316350724973235\t0.1316350724975984\t23.75\t1137327026376875\t1137327026379250\r\n",
      "6478.2\t2457881\t0.1316383624849537\t0.1316383624852720\t27.50\t1137355451870000\t1137355451872750\r\n",
      "6478.1\t2457881\t0.1316383624849248\t0.1316383624853154\t33.75\t1137355451869750\t1137355451873125\r\n"
     ]
    }
   ],
   "source": [
    "!head -8 outputs/combineOut-05072017; echo \"...\" ; tail -5 outputs/combineOut-05072017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12197 outputs/combineOut-05072017\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l outputs/combineOut-05072017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line count looks like what we expect for combining two files of 4424 lines and 7776 lines, respectively.  However, there's one other aspect of this output we might not have expected.  Whenever we've looked at `.thresh` cosmic ray data files before, all of the data has been in chronological order with the earliest muon hits at the beginning and the latest ones at the end.  In the case of this output file, `combineOut-05072017`, though, all of the detector 6148 data is at the beginning of the file, and all of the detector 6478 data is at the end of the file, even though data from the two detectors overlapped in time.  This means that the data can't possibly be in chronological order any more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't a mistake at all, of course.  The `Combine` data transformation does just what its name implies, and only that: it joins two or more files into a single file.  Re-arranging the lines of data to put them in order of time is a separate data transformation handled by a separate script; we'll examine that one soon enough."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
